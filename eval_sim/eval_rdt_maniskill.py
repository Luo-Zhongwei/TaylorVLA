from rich.console import Console

# åˆ›å»ºä¸€ä¸ª Console å®ä¾‹ï¼Œä½ å¯ä»¥æŠŠå®ƒæ”¾åœ¨æ–‡ä»¶çš„é¡¶éƒ¨æˆ–è€…ç±»çš„ __init__ æ–¹æ³•ä¸­
console = Console()
import time

# 1. è®°å½•å¼€å§‹æ—¶é—´
start_total_program_time = time.perf_counter()

from typing import Callable, List, Type
import sys
sys.path.append('/')
import gymnasium as gym
import numpy as np
from mani_skill.envs.sapien_env import BaseEnv
from mani_skill.utils import common, gym_utils
import argparse
import yaml
from scripts.maniskill_model import create_model, RoboticDiffusionTransformerModel
import torch
from collections import deque
from PIL import Image
import cv2
from pathlib import Path


from itertools import product
import pandas as pd
import matplotlib.pyplot as plt
from tqdm.contrib import itertools as tqdm_it
from termcolor import colored, cprint
import random
from datetime import datetime
import inspect, sys, importlib, pathlib


from models.project_enums import PruningMode

from utils.profiler import global_profiler 



# =================================================================
# ========== æ–°å¢ï¼šä¸º Alpha vs Temp æ•°æ®ç”Ÿæˆ Facet Grid çš„ä»£ç  ==========
# =================================================================

def plot_facet_grid_for_alpha_temp(df: pd.DataFrame, env_id: str, keep_tokens: int):
    """
    ä¸ºåŒ…å« alpha å’Œ temp å‚æ•°çš„å®éªŒæ•°æ®ç”Ÿæˆä¸€ä¸ª Facet Grid å›¾è¡¨ã€‚

    Args:
        df (pd.DataFrame): å¿…é¡»åŒ…å« 'alpha', 'temp', 'mean_success_rate', 'std_dev' åˆ—ã€‚
        env_id (str): ç¯å¢ƒIDï¼Œç”¨äºå›¾è¡¨æ ‡é¢˜ã€‚
        keep_tokens (int): ä¿ç•™çš„tokenæ•°é‡ï¼Œç”¨äºå›¾è¡¨æ ‡é¢˜ã€‚
    """
    
    # 1. è·å–æ‰€æœ‰å”¯ä¸€çš„ alpha å€¼ï¼Œä»¥ç¡®å®šéœ€è¦å¤šå°‘ä¸ªå­å›¾
    unique_alphas = sorted(df['alpha'].unique())
    num_alphas = len(unique_alphas)
    
    if num_alphas == 0:
        print("DataFrame ä¸­æœªæ‰¾åˆ° 'alpha' åˆ—ï¼Œæ— æ³•ç”Ÿæˆ Facet Gridã€‚")
        return

    # 2. åŠ¨æ€è®¡ç®—å­å›¾çš„ç½‘æ ¼å¸ƒå±€ï¼ˆä¾‹å¦‚ï¼Œæ¯è¡Œæœ€å¤šæ˜¾ç¤º3ä¸ªå›¾ï¼‰
    ncols = min(3, num_alphas)
    nrows = (num_alphas + ncols - 1) // ncols  # å‘ä¸Šå–æ•´

    # 3. åˆ›å»ºå­å›¾ç½‘æ ¼
    fig, axes = plt.subplots(nrows, ncols, figsize=(5 * ncols, 4 * nrows), squeeze=False)
    
    # å°†äºŒç»´çš„ axes æ•°ç»„å±•å¹³ä¸ºä¸€ç»´ï¼Œæ–¹ä¾¿è¿­ä»£
    axes_flat = axes.flatten()

    # 4. éå†æ¯ä¸ª alpha å€¼ï¼Œåœ¨å¯¹åº”çš„å­å›¾ä¸Šç»˜å›¾
    for i, alpha_val in enumerate(unique_alphas):
        ax = axes_flat[i]
        
        # ç­›é€‰å‡ºå½“å‰ alpha å€¼å¯¹åº”çš„æ•°æ®
        subset_df = df[df['alpha'] == alpha_val]
        
        # åœ¨å½“å‰å­å›¾(ax)ä¸Šç»˜åˆ¶è¯¯å·®æ£’å›¾
        ax.errorbar(
            subset_df['temp'],
            subset_df['mean_success_rate'],
            yerr=subset_df['std_dev'],
            marker='o',
            linestyle='-',
            capsize=4
        )
        
        ax.set_title(f'Alpha = {alpha_val:.2f}', fontsize=12)
        ax.set_xlabel('Temperature (temp)', fontsize=10)
        ax.set_ylabel('Mean Success Rate (%)', fontsize=10)
        ax.grid(True, linestyle='--', linewidth=0.5)

    # 5. å¦‚æœå­å›¾æ•°é‡å°‘äºç½‘æ ¼å¤§å°ï¼Œéšè—å¤šä½™çš„ç©ºå­å›¾
    for j in range(num_alphas, len(axes_flat)):
        axes_flat[j].set_visible(False)

    # 6. æ·»åŠ ä¸€ä¸ªç»Ÿä¸€çš„å¤§æ ‡é¢˜
    fig.suptitle(
        f'Success Rate vs. Temperature (Faceted by Alpha)\n'
        f'Env: {env_id}, Tokens: {keep_tokens}/729',
        fontsize=16,
        weight='bold'
    )

    # 7. è‡ªåŠ¨è°ƒæ•´å¸ƒå±€ï¼Œé˜²æ­¢æ ‡é¢˜å’Œæ ‡ç­¾é‡å 
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    
    # 8. ä¿å­˜å›¾åƒ
    now_str = datetime.now().strftime("%Y%m%d_%H%M%S")
    image_filename = f'{env_id}_{keep_tokens}tokens_facet_grid_{now_str}.png'
    plt.savefig(image_filename, dpi=300)
    print(f"Facet Grid å›¾è¡¨å·²ä¿å­˜ä¸º: {image_filename}")
    plt.close(fig)




def parse_args(args=None):
    parser = argparse.ArgumentParser()
    parser.add_argument("-e", "--env-id", type=str, default="PickCube-v1", help=f"Environment to run motion planning solver on. ")
    parser.add_argument("-o", "--obs-mode", type=str, default="rgb", help="Observation mode to use. Usually this is kept as 'none' as observations are not necesary to be stored, they can be replayed later via the mani_skill.trajectory.replay_trajectory script.")
    parser.add_argument("-n", "--num-traj", type=int, default=25, help="Number of trajectories to test.")
    parser.add_argument("--only-count-success", action="store_true", help="If true, generates trajectories until num_traj of them are successful and only saves the successful trajectories/videos")
    parser.add_argument("--reward-mode", type=str)
    parser.add_argument("-b", "--sim-backend", type=str, default="auto", help="Which simulation backend to use. Can be 'auto', 'cpu', 'gpu'")
    parser.add_argument("--render-mode", type=str, default="rgb_array", help="can be 'sensors' or 'rgb_array' which only affect what is saved to videos")
    parser.add_argument("--shader", default="default", type=str, help="Change shader used for rendering. Default is 'default' which is very fast. Can also be 'rt' for ray tracing and generating photo-realistic renders. Can also be 'rt-fast' for a faster but lower quality ray-traced renderer")
    parser.add_argument("--num-procs", type=int, default=1, help="Number of processes to use to help parallelize the trajectory replay process. This uses CPU multiprocessing and only works with the CPU simulation backend at the moment.")
    parser.add_argument("--pretrained_path", type=str, default=None, help="Path to the pretrained model")
    parser.add_argument("--random_seed", type=int, default=0, help="Random seed for the environment.")
    return parser.parse_args()

import random
import os

# set cuda 
args = parse_args()
# set random seeds
seed = args.random_seed
random.seed(seed)
os.environ['PYTHONHASHSEED'] = str(seed)
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

task2lang = {
    "PegInsertionSide-v1": "Pick up a orange-white peg and insert the orange end into the box with a hole in it.",
    "PickCube-v1": "Grasp a red cube and move it to a target goal position.",
    "StackCube-v1":  "Pick up a red cube and stack it on top of a green cube and let go of the cube without it falling.",
    "PlugCharger-v1": "Pick up one of the misplaced shapes on the board/kit and insert it into the correct empty slot.",
    "PushCube-v1": "Push and move a cube to a goal region in front of it."
}

env_id = args.env_id
env = gym.make(
    env_id,
    obs_mode=args.obs_mode,
    control_mode="pd_joint_pos",
    render_mode=args.render_mode,
    reward_mode="dense" if args.reward_mode is None else args.reward_mode,
    sensor_configs=dict(shader_pack=args.shader),
    human_render_camera_configs=dict(shader_pack=args.shader),
    viewer_camera_configs=dict(shader_pack=args.shader),
    sim_backend=args.sim_backend
)

config_path = 'configs/base.yaml'
with open(config_path, "r") as fp:
    config = yaml.safe_load(fp)
pretrained_text_encoder_name_or_path = "/root/autodl-tmp/ckpt/google/t5-v1_1-xxl"
pretrained_vision_encoder_name_or_path = "/root/autodl-tmp/ckpt/google/siglip-so400m-patch14-384"
pretrained_path = args.pretrained_path
policy = create_model(
    args=config, 
    dtype=torch.bfloat16,
    pretrained=pretrained_path,
    pretrained_text_encoder_name_or_path=pretrained_text_encoder_name_or_path,
    pretrained_vision_encoder_name_or_path=pretrained_vision_encoder_name_or_path
)

if os.path.exists(f'text_embed_{env_id}.pt'):
    text_embed = torch.load(f'text_embed_{env_id}.pt')
else:
    
    torch.cuda.synchronize()  # ç­‰å¾…å½“å‰è®¾å¤‡ä¸Šçš„æ‰€æœ‰æµå®Œæˆ:contentReference[oaicite:0]{index=0}
    start = torch.cuda.Event(enable_timing=True)  # å…è®¸è®¡æ—¶:contentReference[oaicite:1]{index=1}
    end   = torch.cuda.Event(enable_timing=True)
    start.record()

    # ===== è¿™é‡Œæ”¾ç½®ä½ çš„ GPU ä»£ç ç‰‡æ®µ =====
    text_embed = policy.encode_instruction(task2lang[env_id])
    # =====================================
    torch.cuda.synchronize()  # ç­‰å¾…æµä¸­æ‰€æœ‰å†…æ ¸å’Œäº‹ä»¶å®Œæˆ:contentReference[oaicite:2]{index=2}
    end.record()
    # å†æ¬¡åŒæ­¥ï¼Œç¡®ä¿ç»“æŸäº‹ä»¶å·²è¢«è®°å½•
    torch.cuda.synchronize()
    elapsed_ms = start.elapsed_time(end)  # è¿”å›å•ä½ä¸º ms:contentReference[oaicite:3]{index=3}
    from colorama import init
    from termcolor import colored
    init()  # åœ¨ Windows ä¸Šåˆå§‹åŒ–æ”¯æŒ
    print(
        f"policy.encode_instruction GPU time: "
        + colored(f"{elapsed_ms:.3f}", "red")
        + " ms"
    )
    
    torch.save(text_embed, f'text_embed_{env_id}.pt')


print("test inter text encode time")

language_time=0
#calculat text embed time
for i in range(0,30):
    
    language_time=0
    torch.cuda.synchronize()  # ç­‰å¾…å½“å‰è®¾å¤‡ä¸Šçš„æ‰€æœ‰æµå®Œæˆ:contentReference[oaicite:0]{index=0}
    start = torch.cuda.Event(enable_timing=True)  # å…è®¸è®¡æ—¶:contentReference[oaicite:1]{index=1}
    end   = torch.cuda.Event(enable_timing=True)
    start.record()

    # ===== è¿™é‡Œæ”¾ç½®ä½ çš„ GPU ä»£ç ç‰‡æ®µ =====
    text_embed = policy.encode_instruction(task2lang[env_id])
    # =====================================
    torch.cuda.synchronize()  # ç­‰å¾…æµä¸­æ‰€æœ‰å†…æ ¸å’Œäº‹ä»¶å®Œæˆ:contentReference[oaicite:2]{index=2}
    end.record()
    # å†æ¬¡åŒæ­¥ï¼Œç¡®ä¿ç»“æŸäº‹ä»¶å·²è¢«è®°å½•
    torch.cuda.synchronize()
    language_time = start.elapsed_time(end)  # è¿”å›å•ä½ä¸º ms:contentReference[oaicite:3]{index=3}
    
    print(i)
    print(
        f"policy.encode_instruction GPU time: "
        + colored(f"{language_time:.3f}", "red")
        + " ms"
    )
    
    torch.save(text_embed, f'text_embed_{env_id}.pt')


MAX_EPISODE_STEPS = 400 
total_episodes = args.num_traj  
success_count = 0  

base_seed = 20241201
import tqdm
avg_avg_time_list=[]


lzw_log_path="/root/autodl-tmp/RoboticsDiffusionTransformer/lzw_logs/1.txt"

lzw_log_PATH = Path(lzw_log_path)  

#alpha=0.7, temp=0.31622776601683794
alpha= np.linspace(1, 0, num=3)
#alpha = [0.7]
temp = np.logspace(np.log10(0.01), np.log10(10.0), num=3)
#[0.01, 0.05623413, 0.31622777, 1.77827941, 10.]
#temp=[np.logspace(np.log10(0.01), np.log10(10.0), num=5)[3]]
print(temp)
cdp_para=np.linspace(1, 0, num=4)
# 2. æ‰§è¡Œæ¨¡å‹å¹¶æ”¶é›†ç»“æœ
success_data = []
#729  182*1
keep_img_token_nums = 182
#pruning_mode = "no_img_token_pruning" #score
#pruning_mode ="score"

#pruning_mode = PruningMode.CDPRUNER    
pruning_mode = PruningMode.SCORE  # ä½¿ç”¨æšä¸¾æˆå‘˜
#pruning_mode=PruningMode.RANDOM
#pruning_mode=PruningMode.NO_PRUNING
repeat_exp_num=10
for a, b in tqdm_it.product(alpha, temp,
                             desc='grid search'):
#for cdp_it in tqdm.trange(len(cdp_para)):
    rep_suc=[]
    for re_it in tqdm.trange(repeat_exp_num):
        # --- æ ‡è®°ä¸€ä¸ªæ–°çš„ Experiment Run å¼€å§‹ ---
        global_profiler.start_experiment_run(run_idx=re_it) # <--- åœ¨è¿™é‡Œè°ƒç”¨æ–°æ–¹æ³•
        #success_rate=0
        success_count = 0  
        # --- æ ‡è®°ä¸€ä¸ªæ–°çš„ Trial å¼€å§‹ ---
        # --- åœ¨è¿™é‡Œé‡ç½®ç§å­ï¼Œåœ¨æ¯ä¸€æ¬¡è¯•éªŒå¼€å§‹æ—¶ ---
        print(f"Running trial with alpha={a}, temp={b}. Re-seeding with {base_seed}.")
        #print(f"Running trial with cdp_para={cdp_para[cdp_it]},  Re-seeding with {base_seed}.")
        random.seed(base_seed)
        os.environ['PYTHONHASHSEED'] = str(base_seed)
        np.random.seed(base_seed)
        torch.manual_seed(base_seed)
        torch.cuda.manual_seed(base_seed)
    # å¦‚æœæ‚¨ä½¿ç”¨å¤šGPUï¼Œå¯èƒ½è¿˜éœ€è¦ torch.cuda.manual_seed_all(base_script_seed)
        for episode in tqdm.trange(total_episodes):
            global_profiler.start_trial(trial_idx=episode) # <--- åœ¨è¿™é‡Œè°ƒç”¨
            obs_window = deque(maxlen=2)
            obs, _ = env.reset(seed = episode + base_seed)
            policy.reset()
            policy.alpha = a
            policy.temp = b
            policy.keep_img_token_nums=keep_img_token_nums
            policy.vis_count=0
            policy.pruning_mode=pruning_mode
            policy.save_vis_doc_name=str(args.env_id)+str(policy.pruning_mode)+str(keep_img_token_nums)+'_729 token'
            #policy.cdp_para=cdp_para[cdp_it]
            policy.cdp_para=None
            
            img = env.render().squeeze(0).detach().cpu().numpy()
            obs_window.append(None)
            obs_window.append(np.array(img))
            proprio = obs['agent']['qpos'][:, :-1]

            global_steps = 0
            video_frames = []

            success_time = 0
            done = False
            
            while global_steps < MAX_EPISODE_STEPS and not done:
                image_arrs = []
                for window_img in obs_window:
                    image_arrs.append(window_img)
                    image_arrs.append(None)
                    image_arrs.append(None)
                images = [Image.fromarray(arr) if arr is not None else None
                        for arr in image_arrs]
                #action [64,8] 
                actions = policy.step(proprio, images, text_embed).squeeze(0).cpu().numpy()
                
                # Take 8 steps since RDT is trained to predict interpolated 64 steps(actual 14 steps)
                #[16,8]
                actions = actions[::4, :]
                
                for idx in range(actions.shape[0]):
                    action = actions[idx]
                    obs, reward, terminated, truncated, info = env.step(action)
                    img = env.render().squeeze(0).detach().cpu().numpy()
                    obs_window.append(img)
                    proprio = obs['agent']['qpos'][:, :-1]
                    video_frames.append(img)
                    global_steps += 1
                    if terminated or truncated:
                        assert "success" in info, sorted(info.keys())
                        if info['success']:
                            success_count += 1
                            done = True
                            break 
            print(f"Trial {episode+1} finished, success: {info['success']}, steps: {global_steps}")
            print(f"policy.alpha = {policy.alpha}"+f"policy.temp = {policy.temp}")
            
            
            
                
            
            #avg_diffuion_time = sum(policy.runing_time) / len(policy.runing_time)
            #print("avg_diffuion_time: ",avg_diffuion_time)  
            #content  = f"{str(policy.runing_time)}\n"    
            # precision = 3                                 # æƒ³ä¿ç•™çš„å°æ•°ä½
            # formatted = [f"{t:.{precision}f}" for t in (policy.runing_time)]
            # content   = f"[{', '.join(formatted)}]\n"     
            # content += f"avg_diffuion_time={avg_diffuion_time:.3f}\n"           # => score=0.93   + æ¢è¡Œ
            # with lzw_log_PATH.open('a', encoding='utf-8') as f:
            #     f.write(content)
            
            
            #avg_avg_time_list.append(avg_diffuion_time)
        #success_rate=8 +random.uniform(1.5, 2) 
        success_rate = success_count / total_episodes * 100
    #print(f"Success rate: {success_rate}%")
        print(f"policy.alpha = {policy.alpha}"+f"policy.temp = {policy.temp}")
        s = colored(f"Success rate: {success_rate}%", "yellow", attrs=["bold"])
        print(s)
        rep_suc.append(success_rate)
    print(f"rep Success rate: {rep_suc}%")
    # --- å½“ä¸€ç»„é‡å¤å®éªŒ (repeat_exp_num) å…¨éƒ¨ç»“æŸå ---
    # 1. å°†åˆ—è¡¨è½¬æ¢ä¸º NumPy æ•°ç»„ä»¥ä¾¿é«˜æ•ˆè®¡ç®—
    rep_suc_array = np.array(rep_suc)
    #    æ³¨æ„: å½“é‡å¤æ¬¡æ•° > 1 æ—¶ï¼Œè¿™äº›è®¡ç®—æ‰æœ‰æ„ä¹‰
    if len(rep_suc_array) > 1:
        mean_rate = np.mean(rep_suc_array)
        # ä½¿ç”¨æ ·æœ¬æ ‡å‡†å·®(ddof=1)æ›´ç¬¦åˆç»Ÿè®¡æƒ¯ä¾‹ï¼Œå› ä¸ºå®ƒèƒ½æ›´å¥½åœ°ä¼°è®¡æ€»ä½“æ ‡å‡†å·®
        std_dev = np.std(rep_suc_array, ddof=1) 
        variance = np.var(rep_suc_array, ddof=1)
    else: # å¦‚æœåªé‡å¤ä¸€æ¬¡ï¼Œæ ‡å‡†å·®å’Œæ–¹å·®ä¸º0
        mean_rate = rep_suc_array[0] if len(rep_suc_array) > 0 else 0
        std_dev = 0
        variance = 0
    
    print(f"å‚æ•°ç»„ (alpha={a}, temp={b}) çš„ç»“æœ:")
    #print(f"å‚æ•°ç»„ (cdp_para[cdp_it]={cdp_para[cdp_it]}) çš„ç»“æœ:")
    print(f"  - åŸå§‹æˆåŠŸç‡åˆ—è¡¨: {rep_suc}")
    cprint(f"  - å¹³å‡æˆåŠŸç‡ (Mean): {mean_rate:.2f}%", "cyan", attrs=["bold"])
    print(f"  - æ ‡å‡†å·® (Std Dev): {std_dev:.2f}")
    print(f"  - æ–¹å·® (Variance): {variance:.2f}")
    # 4. åˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰ä¿¡æ¯çš„å­—å…¸
    # result_entry = {
        
    #     'cdp_para[cdp_it]': cdp_para[cdp_it],
    #     'mean_success_rate': mean_rate,
    #     'std_dev': std_dev,
    #     'variance': variance,
    #     'raw_success_rates': rep_suc  # ä¿å­˜åŸå§‹æ•°æ®åˆ—è¡¨
    # }
    result_entry = {
            'alpha': a,
            'temp': b,
            'mean_success_rate': mean_rate,
            'std_dev': std_dev,
            'variance': variance,
            'raw_success_rates': rep_suc
        }
    success_data.append(result_entry)
    

    # avg_avg_diffuion_time = sum(avg_avg_time_list) / len(avg_avg_time_list)
    # precision = 3                                 # æƒ³ä¿ç•™çš„å°æ•°ä½
    # formatted = [f"{t:.{precision}f}" for t in (avg_avg_time_list)]
    # content   = f"[{', '.join(formatted)}]\n" 
    # content += f"avg_avg_diffuion_time={avg_avg_diffuion_time}\n"           # => score=0.93   + æ¢è¡Œ
    # with lzw_log_PATH.open('a', encoding='utf-8') as f:
    #     f.write(content)

    
    #print(f"avg_avg_diffuion_time: {avg_avg_diffuion_time}ms")
    
    
    
#old df for a b and score pruning  
df = pd.DataFrame(success_data)
# # å°†åŸå§‹æ•°æ®åˆ—è¡¨ä¹Ÿä¿å­˜åˆ° Excelï¼Œæ³¨æ„å®ƒä¼šä»¥å­—ç¬¦ä¸²å½¢å¼ä¿å­˜
df.to_excel('results_with_stats.xlsx', index=False)

# # ====================== ä¼˜åŒ–åçš„ç»˜å›¾ä»£ç  ======================
# # å°† DataFrame ä¿å­˜åˆ° Excel/CSVï¼Œå»ºè®®æ–‡ä»¶åä¸å›¾ç‰‡æ–‡ä»¶åä¿æŒä¸€è‡´ï¼Œæ–¹ä¾¿è¿½æº¯
# now_str = datetime.now().strftime("%Y%m%d_%H%M%S")
# excel_filename = f'results_{args.env_id}_{keep_img_token_nums}tokens_{now_str}.xlsx'
# df.to_excel(excel_filename, index=False)
# print(f"å®éªŒç»“æœå·²ä¿å­˜è‡³: {excel_filename}")

# # --- å¼€å§‹ç»˜å›¾ ---
# plt.figure(figsize=(12, 8)) # åˆ›å»ºä¸€ä¸ªæ›´å¤§ã€æ¯”ä¾‹æ›´å¥½çš„ç”»å¸ƒ

# # è®¾ç½®ä¸€ä¸ªæ›´ç¾è§‚çš„ç»˜å›¾é£æ ¼
# plt.style.use('seaborn-v0_8-whitegrid')

# # éå†æ¯ä¸ª alpha å€¼æ¥ç»˜åˆ¶æ›²çº¿
# for a_val in sorted(df['alpha'].unique()):
#     sub_df = df[df['alpha'] == a_val]
#     # ä½¿ç”¨ errorbar åŒæ—¶ç»˜åˆ¶å‡å€¼çº¿å’Œæ ‡å‡†å·®èŒƒå›´ï¼Œè¿™èƒ½æ›´å¥½åœ°å±•ç¤ºæ•°æ®çš„ç¨³å®šæ€§
#     plt.errorbar(
#         sub_df['temp'], 
#         sub_df['mean_success_rate'], 
#         yerr=sub_df['std_dev'],  # yerr å‚æ•°ç”¨æ¥æ˜¾ç¤ºæ ‡å‡†å·®
#         marker='o',             # æ•°æ®ç‚¹æ ·å¼
#         linestyle='-',          # çº¿æ¡æ ·å¼
#         capsize=4,              # è¯¯å·®æ£’é¡¶éƒ¨çš„æ¨ªçº¿å®½åº¦
#         label=f'alpha={a_val}'
#     )

# # --- è®¾ç½®å›¾è¡¨çš„å„ç§æ ‡ç­¾å’Œæ ‡é¢˜ ---
# plt.xlabel('Temperature (temp)', fontsize=12)
# plt.ylabel('Mean Success Rate (%)', fontsize=12)
# # ä½¿ç”¨ f-string åŠ¨æ€ç”Ÿæˆæ ‡é¢˜ï¼Œæ›´æ¸…æ™°
# title = (
#     f"Success Rate vs. Temperature for different Alpha values\n"
#     f"Env: {args.env_id}, Pruning: {policy.pruning_mode.name}, Tokens: {keep_img_token_nums}/729"
# )
# plt.title(title, fontsize=14, weight='bold')
# plt.legend(title='Alpha Values', fontsize=10) # ç»™å›¾ä¾‹ä¹ŸåŠ ä¸Šæ ‡é¢˜
# plt.grid(True, which='both', linestyle='--', linewidth=0.5) # æ·»åŠ ç½‘æ ¼çº¿

# plt.tight_layout() # è‡ªåŠ¨è°ƒæ•´å¸ƒå±€ï¼Œé˜²æ­¢æ ‡ç­¾é‡å 

# # --- ã€å…³é”®æ­¥éª¤ã€‘ä¿å­˜å›¾åƒæ–‡ä»¶ ---
# # ä½¿ç”¨åŠ¨æ€æ–‡ä»¶åï¼ŒåŒ…å«ç¯å¢ƒIDå’Œæ—¶é—´æˆ³ï¼Œé¿å…è¦†ç›–
# image_filename = f'{args.env_id}_{keep_img_token_nums}tokens_{now_str}_{pruning_mode.name}.png'
# plt.savefig(image_filename, dpi=300) # dpi=300 å¯ä»¥è·å¾—é«˜åˆ†è¾¨ç‡å›¾åƒ
# print(f"ç»˜å›¾å·²ä¿å­˜ä¸º: {image_filename}")

# # ... ä¿å­˜å›¾ç‰‡çš„ä»£ç ä¸å˜ ...

# s = colored(str(success_data), "yellow", attrs=["bold"])
# print(s)

# # æŒ‰å¹³å‡æˆåŠŸç‡æ‰¾åˆ°æœ€ä½³ç»“æœ
# best_rate = max(item['mean_success_rate'] for item in success_data)

# # æ‰“å°æ‰€æœ‰ç­‰äºæœ€å¤§å€¼çš„é¡¹ï¼Œå¹¶æ˜¾ç¤ºæ›´ä¸°å¯Œçš„ä¿¡æ¯
# for it in success_data:
#     if it['mean_success_rate'] == best_rate:
#         print(colored("="*20 + " BEST RESULT " + "="*20, "green"))
#         print(f"  - Parameters: alpha={it['alpha']}, temp={it['temp']}")
#         print(f"  - Mean Success Rate: {it['mean_success_rate']:.2f}%")
#         print(f"  - Standard Deviation: {it['std_dev']:.2f}")
#         print(f"  - Raw Data: {it['raw_success_rates']}")
#         print(colored("="*53, "green"))

# print("policy.keep_img_token_nums:",policy.keep_img_token_nums)
# print("policy.save_vis_doc_name:",policy.save_vis_doc_name)
# print("policy.pruning_mode:",policy.pruning_mode)
# print("repeat_exp_num: ",repeat_exp_num)
# now = datetime.now()

# # æ ¼å¼åŒ–ä¸º å¹´æ—¥æœˆå°æ—¶åˆ†
# print(now.strftime("%Y-%d-%m %H:%M"))
# # 2. è®°å½•ç»“æŸæ—¶é—´
# end_time = time.perf_counter()

# # 3. è®¡ç®—å¹¶æ‰“å°æµé€æ—¶é—´
# elapsed_time = end_time - start_total_program_time
# print(f"ç¨‹åºæ€»å…±è€—æ—¶: {elapsed_time:.4f} ç§’")

##########################


# ====================== ä¼˜åŒ–åçš„ç»˜å›¾ä»£ç  (é’ˆå¯¹CDPå‚æ•°) ======================
# å°† DataFrame ä¿å­˜åˆ° Excel/CSV
now_str = datetime.now().strftime("%Y%m%d_%H%M%S")
excel_filename = f'results_{args.env_id}_{pruning_mode.name}_{keep_img_token_nums}tokens_{now_str}.xlsx'
df.to_excel(excel_filename, index=False)
print(f"å®éªŒç»“æœå·²ä¿å­˜è‡³: {excel_filename}")


# ====================== ä¿®æ­£åçš„â€œæœ€ä½³ç»“æœâ€æ‰“å°éƒ¨åˆ† ======================
s = colored(str(success_data), "yellow", attrs=["bold"])
print(s)

# æŒ‰å¹³å‡æˆåŠŸç‡æ‰¾åˆ°æœ€ä½³ç»“æœ
# æ³¨æ„ï¼šå¦‚æœæˆåŠŸç‡æœ‰å¤šä¸ªå¹¶åˆ—æœ€é«˜ï¼Œè¿™é‡Œä¼šæ‰“å°æ‰€æœ‰å¹¶åˆ—æœ€é«˜çš„é¡¹
best_rate = df['mean_success_rate'].max()
best_results_df = df[df['mean_success_rate'] == best_rate]
plot_facet_grid_for_alpha_temp(df, args.env_id, keep_img_token_nums)
print(colored("\n" + "="*20 + " BEST RESULT(S) " + "="*20, "green"))
for index, row in best_results_df.iterrows():
    # MODIFIED: æ‰“å°æ­£ç¡®çš„å‚æ•°
    print(f"  - Parameters: alpha={row['alpha']:.3f}, temp={row['temp']:.4f}")
    print(f"  - Mean Success Rate: {row['mean_success_rate']:.2f}%")
    print(f"  - Standard Deviation: {row['std_dev']:.2f}")
    print(f"  - Raw Data: {row['raw_success_rates']}")
    print("-" * 54)
print(colored("="*54, "green"))


print("policy.keep_img_token_nums:",policy.keep_img_token_nums)
print("policy.save_vis_doc_name:",policy.save_vis_doc_name)
print("policy.pruning_mode:",policy.pruning_mode)
print("repeat_exp_num: ",repeat_exp_num)
now = datetime.now()

# æ ¼å¼åŒ–ä¸º å¹´æ—¥æœˆå°æ—¶åˆ†
print(now.strftime("%Y-%d-%m %H:%M"))
# 2. è®°å½•ç»“æŸæ—¶é—´
end_time = time.perf_counter()

# 3. è®¡ç®—å¹¶æ‰“å°æµé€æ—¶é—´
elapsed_time = end_time - start_total_program_time
print(f"ç¨‹åºæ€»å…±è€—æ—¶: {elapsed_time:.4f} ç§’")
# =================================================================
# ========== åœ¨æ‰€æœ‰å¤§å¾ªç¯ç»“æŸåï¼Œåœ¨è¿™é‡Œè¿›è¡Œæœ€ç»ˆæ€§èƒ½æ€»ç»“ ==========
# =================================================================

print("\n" + "="*80)
print("ğŸ æ‰€æœ‰å®éªŒå·²å®Œæˆï¼Œæ­£åœ¨ç”Ÿæˆæœ€ç»ˆæ€§èƒ½æ€»ç»“æŠ¥å‘Š...")
print("="*80)

# 1. ç”ŸæˆæŠ¥å‘Š
# è¿™ä¸ªå‡½æ•°ä¼šå¤„ç†æ‰€æœ‰æ”¶é›†åˆ°çš„æ•°æ®ï¼Œå¹¶è®¡ç®—å‡ºæ€»ä½“å¹³å‡å€¼å’Œæ ‡å‡†å·®
final_report = global_profiler.generate_report()

# 2. ä»¥æ˜“äºé˜…è¯»çš„æ–¹å¼æ˜¾ç¤ºæŠ¥å‘Š
# æ‚¨éœ€è¦çš„æ‰€æœ‰ä¿¡æ¯ï¼ˆæ€»ä½“å‡å€¼ã€æ ‡å‡†å·®ï¼‰éƒ½ä¼šåœ¨è¿™é‡Œæ¸…æ™°åœ°æ‰“å°å‡ºæ¥
global_profiler.display_report(final_report)

# 3. (å¯é€‰) å¦‚æœæ‚¨æƒ³åœ¨ä»£ç ä¸­ç›´æ¥ä½¿ç”¨è¿™äº›å€¼
if "overall_summary" in final_report and final_report["overall_summary"]:
    overall_stats = final_report["overall_summary"]
    print("\n--- æå–å…³é”®æ€§èƒ½æŒ‡æ ‡ ---")
    
    if "Vision" in overall_stats:
        vision_stats = overall_stats["Vision"]
        print(f"ğŸ‘ï¸ Vision æ—¶é—´:")
        print(f"  - æ€»ä½“å¹³å‡å€¼ (Mean): {vision_stats['mean_of_means']:.2f} ms")
        print(f"  - æ€»ä½“æ ‡å‡†å·® (Std Dev): {vision_stats['std_of_means']:.2f} ms")
    
    if "Action" in overall_stats:
        action_stats = overall_stats["Action"]
        print(f"ğŸƒ Action æ—¶é—´:")
        print(f"  - æ€»ä½“å¹³å‡å€¼ (Mean): {action_stats['mean_of_means']:.2f} ms")
        print(f"  - æ€»ä½“æ ‡å‡†å·® (Std Dev): {action_stats['std_of_means']:.2f} ms")
    # è¿™é‡Œçš„ count æ˜¯æ‚¨è¿è¡Œçš„æ€» run æ¬¡æ•° (4 * 20 = 80)
    print(f"\n* ä»¥ä¸Šç»Ÿè®¡æ•°æ®åŸºäºå…¨éƒ¨ {overall_stats['Vision']['count']} æ¬¡ç‹¬ç«‹å®éªŒè¿è¡Œ (experiment run) çš„ç»“æœã€‚")